# ğŸ“„ RAG Chatbot for eBay User Agreement â€” Amlgo Labs Assignment

## ğŸš€ Project Overview
This is an AI-powered chatbot capable of answering user queries based on the eBay User Agreement document. It uses a Retrieval-Augmented Generation (RAG) pipeline with a vector database and a language model (LLM).

The chatbot supports real-time streaming responses via a Streamlit interface and retrieves information grounded in the provided document.

---

## ğŸ› ï¸ Tech Stack
- **Embedding Model:** `all-MiniLM-L6-v2`
- **LLM:** `google/flan-t5-large`
- **Vector Database:** FAISS
- **Frontend:** Streamlit (with streaming response)
- **Programming Language:** Python

---

## âœ… Features
- ğŸ” Document-aware, accurate answers
- ğŸ”— Shows source text chunks used for generating the answer
- ğŸ§  Memory of chat history during the session
- ğŸš€ Real-time streaming responses
- ğŸ–¥ï¸ Simple and clean web interface

---

## ğŸ“ Folder Structure

RAG-Chatbot-eBay/
â”œâ”€â”€ data/ â†’ Raw document (eBay user agreement)
â”œâ”€â”€ vectordb/ â†’ FAISS index and saved chunks
â”œâ”€â”€ notebooks/ â†’ Jupyter notebook for preprocessing & vector DB creation
â”œâ”€â”€ app.py â†’ Streamlit chatbot app
â”œâ”€â”€ requirements.txt â†’ Python dependencies
â””â”€â”€ README.md â†’ Project documentation


---

## ğŸ”§ Installation & Running

### âœ… 1. Clone the repository:

```bash
git clone https://github.com/your-username/RAG-Chatbot-eBay.git
cd RAG-Chatbot-eBay


### âœ… 2. Create virtual environment:

python -m venv ragenv
.\ragenv\Scripts\activate

### âœ… 3. Install dependencies:

pip install -r requirements.txt


### âœ… 4. Run the Streamlit app:

streamlit run app.py


### âœ… 5. Open your browser at:

http://localhost:8501



ğŸ” Sample Queries
What are the rules for selling motor vehicles on eBay?

# Can I cancel a transaction as a buyer?

# What happens if I violate eBay's terms?

# Is arbitration mandatory for disputes?


ğŸ§  How it Works
âœ… The document is chunked into 100â€“300 word segments using sentence-aware splitting.

âœ… Each chunk is embedded into a vector using all-MiniLM-L6-v2.

âœ… FAISS vector DB is used to retrieve top matching chunks based on the user query.

âœ… Retrieved chunks + user question are injected into a prompt.

âœ… The LLM (google/flan-t5-large) generates an answer grounded in the context.


âš ï¸ Limitations
# Model responses depend on the relevance of retrieved chunks.

# Answers are limited to the knowledge present in the provided document.

# Some long or vague queries may produce less accurate results.


ğŸ“œ License
This project is created as part of the Amlgo Labs Junior AI Engineer assignment.


ğŸ™‹â€â™‚ï¸ Author
Naushad Alam
Email: naushadlil01@gmail.com
GitHub: github.com/Naushad-Alam01